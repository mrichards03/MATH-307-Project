\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\title{MATH 307-101
Applied Linear Algebra
2024 Winter Term 1 (Sepâ€“Dec 2024)}
\author{MacKenzie Richards, Ankkit Prakash, Yuxin Hu, \\ Somesh Joshi, Cai lewendon}
\date{December 2024}
\begin{document}
\maketitle
\section{Approximating Eigenvalues with the QR Algorithm}
\subsection{Prove $A_1$ is similar to A}
    A is a square matrix. Let, $A = QR$ be the QR factorization of A. \\
    Q is a square matrix, and its columns form an orthonormal basis by construction. \\
    Therefore, $Q^{-1} = Q^T$ and $Q^{T}Q = QQ^T = I$, i.e, Q is invertible\\
    $A_1 = RQ$.\\ 
    A matrix $A$ is similar to another matrix $B$ if there exists an invertible matrix $P$ such that:
    $$A = PBP^{-1}$$
    For $B = A_1$ and $P = Q$ \\
    $$A = QA_{1}Q^T = Q (RQ) Q^T = QR$$ \\
    Therefore, $A_1$ is similar to $A$. \\ 
    Now, to prove $A_1$ and $A$ have the same eigenvectors. \\
    A matrix has an eigenvalue $\lambda$ and corresponding eigenvector $v$ if \\
    $$Av = \lambda v$$
    Since $A_1$ is similar to $A$
    $$Q (RQ) Q^T v = \lambda v$$
    Left multiply both sides by $Q^T$
    $$ (Q^{T}Q) (RQ) (Q^{T}v) = Q^{T} \lambda v = \lambda (Q^{T}v)$$ \\
    Set $u = Q^{T} v$, and since $A_1 = RQ$. We get, \\
    $$ A_1 u = \lambda u$$
    Since $\lambda$ and $v$ were arbitrary, we have shown that $A$ and $A_1$ have the same eigenvalues.

\subsection{Find $A_1$ and $A$, verify they have the same eigenvalues}

$$
A = \begin{bmatrix}
1 & 0 \\
1 & 3 
\end{bmatrix} 
$$

    Finding the eigenvalues from factoring the characteristic equation, we get \\
    $$det(A - \lambda I) = (1 - \lambda)(3 - \lambda) = 0 \implies \lambda = 3,1$$ \\

    The QR factorization of A results in $A = QR$ such that \\
    $$
    Q = \begin{bmatrix}
        \frac{1}{\sqrt{2}} & \frac{-3}{\sqrt{18}} \\
        \frac{1}{\sqrt{2}} & \frac{3}{\sqrt{18}} 
        \end{bmatrix},
    R = \begin{bmatrix}
        \sqrt{2} & \frac{3}{\sqrt{2}} \\
        0 & \frac{\sqrt{18}}{2} 
        \end{bmatrix}
    $$ \\
    Therefore, $A_1 = RQ$ is \\
    $$
    A_1 = RQ = 
    \begin{bmatrix}
        \frac{1}{\sqrt{2}} & \frac{-3}{\sqrt{18}} \\
        \frac{1}{\sqrt{2}} & \frac{3}{\sqrt{18}} 
    \end{bmatrix}
    \begin{bmatrix}
        \sqrt{2} & \frac{3}{\sqrt{2}} \\
        0 & \frac{\sqrt{18}}{2} 
    \end{bmatrix}
    = \frac{1}{2}
     \begin{bmatrix}
        5 & 1 \\
        3 & 3 
    \end{bmatrix}
    $$
    Finding the eigenvalues from factoring the characteristic equation of $A_1$, we get \\
    $$det(A_1 - \lambda I) = (\frac{5}{2} - \lambda)(\frac{3}{2} - \lambda) - \frac{3}{2 \times 2} = 0 \implies \lambda^2 -4\lambda +3 = 0  \\
    $$
    $$\implies (1 - \lambda)(3 - \lambda) = 0 \implies \lambda = 3,1$$ \\\\

    Therefore, $A_1$ and $A$ have the same eigenvalues


\subsection{$A_k$ is similar to $A$}

Proof by induction. \\
Inductive Hypothesis: $A_k$ is similar to $A$ for $k \geq 1$ \\
Base case:$A_1$ is similar to $A$ \\
For an invertible matrix Q, \\
$A = QA_{1}Q^T = Q (RQ) Q^T = QR$ \\ \\
Assume inductive hypothesis is true, that $A_k$ is similar to $A$ for $k \geq 1$ \\ \\
To show: $A_{k+1}$ is similar to $A_k$, and therefore, $A$ \\

$A_{k+1} = R_kQ_k$ 

$A_k = Q_k A_{k+1} Q_k^T  =  Q_k R_kQ_k Q_k^T = Q_k R_k$

Clearly, $A_{k+1}$ is similar to $A_k$. Therefore, $A_{k+1}$ is similar to $A_k$ (since $A_k = Q_kR_k = R_{k-1}Q_{k-1}$ and so forth) by the inductive hypothesis.




\subsection{Continuing Problem  1.2}
Python code was written to calculate $A_2, A_3, A_4, A_5$. \\
$$
A_2 = \begin{bmatrix}
3.12 & -0.47 & \\
0.53 & 0.88 & \\
\end{bmatrix}
A_3 = \begin{bmatrix}
3.06 & 0.84 & \\
-0.16 & 0.94 & \\
\end{bmatrix}
A_4 = \begin{bmatrix}
3.02 & -0.95 & \\
0.05 & 0.98 & \\
\end{bmatrix}
A_5 = \begin{bmatrix}
3.01 & 0.98 & \\
-0.02 & 0.99 & \\
\end{bmatrix}
$$ \\ 

As we can clearly see, $A_k \xrightarrow{} A$ as $k$ increases


\subsection{}
\subsection{QR Algorithm in Python}
    Results from python code (QRAlgorithm(1.6-1.8).py): \\
    \begin{tabular}{c|c}
        \textbf{Matrix} & \textbf{Eigenvalues} \\
        \hline \\
        \(\begin{bmatrix} 2 & 3 \\ 2 & 1 \end{bmatrix}\) & 4.0, -1.0 \\
        \\ \hline \\
        \(\begin{bmatrix} 1 & 1 \\ 2 & 1 \end{bmatrix}\) & 2.41, -0.41 \\
        \\ \hline \\
        \(\begin{bmatrix} 1 & 0 & -1 \\ 1 & 2 & 1 \\ -4 & 0 & 1 \end{bmatrix}\) & 3.01, 1.99, -1.0 \\
        \\ \hline \\
        \(\begin{bmatrix} 1 & 1 & -1 \\ 0 & 2 & 0 \\ -2 & 4 & 2 \end{bmatrix}\) & 3.0, 2.0, 0.0 \\
    \end{tabular}

\subsection{Another Example}
    Results from python code (QRAlgorithm(1.6-1.8).py): \\
    \begin{tabular}{c|c}
        \textbf{Matrix} & \textbf{Eigenvalues} \\
        \hline \\
        \(\begin{bmatrix} 2 & 3 \\ -1 & -2 \end{bmatrix}\) & 2.0, -2.0 \\
    \end{tabular} \\ \\
    This is clearly incorrect as the eigenvalues should be 1 and -1. The basic QR algorithm has failed because the absolute values of the eigenvalues are non-distinct. 
    The QR algorithm relies on seperating eigenvalues based on their magnitudes and since the eigenvalues are the same magnitude, the algorithm fails.
\subsection{Using a Shift}
    Results from python code (QRAlgorithm(1.6-1.8).py): \\
    \begin{tabular}{c|c}
        \textbf{Matrix} & \textbf{Eigenvalues} \\
        \hline \\
        \(\begin{bmatrix} 2 & 3 \\ -1 & -2 \end{bmatrix}\) & 1.0, -1.0 \\
    \end{tabular} \\ \\ \\
$B = A + \alpha I \implies \lambda$ is an eigenvalue of A $\iff \lambda + \alpha$ is an eigenvalue of B. 
\begin{proof}
    $ $\newline
    ($\implies$) Let $\lambda$ be an eigenvalue of A. Then there exists a non-zero vector x such that $Ax = \lambda x$. \\
    Then $Bx = (A + \alpha I)x = Ax + \alpha x = \lambda x + \alpha x = (\lambda + \alpha)x$. \\ 
    Thus, $\lambda + \alpha$ is an eigenvalue of B. \\ \\
    ($\impliedby$) Let $\lambda + \alpha$ be an eigenvalue of B. Then there exists a non-zero vector x such that $Bx = (\lambda + \alpha)x$. \\
    Then $Ax = (B - \alpha I) x = Bx - \alpha I x = (\lambda + \alpha)x - \alpha x = \lambda x$. \\
    Thus, $\lambda$ is an eigenvalue of A. \\ 
\end{proof}


\subsection{The $QR$ factorization of $A^{k+1}$}
\begin{proof}
    $ $\newline
    Let $Q_0 = Q$ and $R_0 = R$. \\
    \textbf{1.} $Q_0 Q_1 \cdots Q_{k-1} A_k = A Q_0 Q_1 \cdots Q_{k-1}$ for all $k \geq 1$ \\ \\
    \textit{Base Case:} $k = 1$ so $A = Q_0 R_0, A_1 = R_0 Q_0$ then, \\
    \indent $Q_0 A_1 = (Q_0 R_0) Q_0 = A Q_0$ \\
    \textit{Inductive Step:} Assume $Q_0 Q_1 \cdots Q_{k-1} A_k = Q_0 Q_1 \cdots Q_{k-1} R_{k-1} Q_{k-1} = A Q_0 Q_1 \cdots Q_{k-1}$, then for $k+1$ \\
    \indent $Q_0 Q_1 \cdots Q_{k-1} Q_k A_{k+1} = Q_0 Q_1 \cdots Q_{k-1} Q_k R_k Q_k$ \\ 
    \indent $= Q_0 Q_1 \cdots Q_{k-1} A_k Q_k$ (Since $A_k = Q_k R_k$) \\
    \indent $= (Q_0 Q_1 \cdots Q_{k-1} R_{k-1} Q_{k-1}) Q_k$ (Since $A_k = R_{k-1} Q_{k-1}$) \\
    \indent $= A Q_0 Q_1 \cdots Q_{k-1} Q_k$ (By Inductive Hypothesis)\\ \\
    Thus $Q_0 Q_1 \cdots Q_{k-1} A_{k} = A Q_0 Q_1 \cdots Q_{k-1}$. \\ \\
    \textbf{2.} $(Q_0 Q_1 \cdots Q_k)(R_k \cdots R_1 R_0) = A(Q_0 Q_1 \cdots Q_{k-1})(R_{k-1} \cdots R_1 R_0)$ \\ \\
    \textit{Base Case:} $k = 1$ so $A = Q_0 R_0, A_1 = R_0 Q_0$ then, \\
    \indent $(Q_0 Q_1)(R_1 R_0) = Q_0 Q_1 R_1 R_0 = Q_0 A_1 R_0$ (Since $A_1 = Q_1 R_1$) \\
    \indent $= Q_0 R_0 Q_0 R_0$ (Since $A_1 = R_0 Q_0$) \\ 
    \indent $= A Q_0 R_0$ (Since $A = Q_0 R_0$) \\
    \textit{Inductive Step:} Assume $(Q_0 Q_1 \cdots Q_k)(R_k \cdots R_1 R_0) = A(Q_0 Q_1 \cdots Q_{k-1})(R_{k-1} \cdots R_1 R_0)$, then for $k+1$ \\
    \indent $(Q_0 Q_1 \cdots Q_k Q_{k+1})(R_{k+1} R_k \cdots R_1 R_0) = Q_0 Q_1 \cdots Q_k Q_{k+1}R_{k+1} R_k \cdots R_1 R_0$ \\
    \indent $= Q_0 Q_1 \cdots Q_k A_{k+1} R_k \cdots R_1 R_0$ (Since $A_{k+1} = Q_{k+1} R_{k+1}$) \\
    \indent $= Q_0 Q_1 \cdots Q_k R_k Q_k R_k \cdots R_1 R_0$ (Since $A_{k+1} = R_k Q_k$) \\
    \indent $= (Q_0 Q_1 \cdots Q_{k-1} A_k) Q_k R_k R_{k-1} \cdots R_1 R_0$ (Since $A_k = Q_k R_k$) \\
    \indent $= (AQ_0 Q_1 \cdots Q_{k-1}) Q_k R_k R_{k-1} \cdots R_1 R_0$ (By \textbf{1.}) \\
    \indent $= A(Q_0 Q_1 \cdots Q_{k-1} Q_k) (R_k R_{k-1} \cdots R_1 R_0)$ \\ \\
    Thus $(Q_0 Q_1 \cdots Q_k)(R_k \cdots R_1 R_0) = A(Q_0 Q_1 \cdots Q_{k-1})(R_{k-1} \cdots R_1 R_0)$. \\ \\
    \textbf{3.} $A^{k+1} = (Q_0 Q_1 \cdots Q_k) (R_k \cdots R_1 R_0)$ \\ \\
    \textit{Base Case:} $k = 0$ so $A^1 = Q_0 R_0 = QR = A$ \\
    \textit{Inductive Step:} Assume $A^{k+1} = (Q_0 Q_1 \cdots Q_k) (R_k \cdots R_1 R_0)$, then for $k+2$ \\
    \indent $A^{k+2} = A A^{k+1} = A (Q_0 Q_1 \cdots Q_k) (R_k \cdots R_1 R_0)$ (By Inductive Hypothesis) \\
    \indent $= (Q_0 Q_1 \cdots Q_kQ_{k+1})(R_{k+1}R{k} \cdots R_1 R_0)$ (By \textbf{2.}) \\ \\
    Thus $A^{k+1} = (Q_0 Q_1 \cdots Q_k) (R_k \cdots R_1 R_0)$ for all $k \geq 0$.
\end{proof}


\end{document}
