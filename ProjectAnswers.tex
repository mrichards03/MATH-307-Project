\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}

\title{MATH 307-101
Applied Linear Algebra}
\author{MacKenzie Richards, Ankkit Prakash, \\ Somesh Joshi, Cai Lewendon}
\date{2024 Winter Term 1 (Sepâ€“Dec 2024)}
\begin{document}
\maketitle
\section{Approximating Eigenvalues with the QR Algorithm}
\subsection{Prove $A_1$ is similar to A}
    A is a square matrix. Let, $A = QR$ be the QR factorization of A. \\
    Q is a square matrix, and its columns form an orthonormal basis by construction. \\
    Therefore, $Q^{-1} = Q^T$ and $Q^{T}Q = QQ^T = I$, i.e, Q is invertible\\
    $A_1 = RQ$.\\ 
    A matrix $A$ is similar to another matrix $B$ if there exists an invertible matrix $P$ such that:
    $$A = PBP^{-1}$$
    For $B = A_1$ and $P = Q$ \\
    $$A = QA_{1}Q^T = Q (RQ) Q^T = QR$$ \\
    Therefore, $A_1$ is similar to $A$. \\ 
    Now, to prove $A_1$ and $A$ have the same eigenvectors. \\
    A matrix has an eigenvalue $\lambda$ and corresponding eigenvector $v$ if \\
    $$Av = \lambda v$$
    Since $A_1$ is similar to $A$
    $$Q (RQ) Q^T v = \lambda v$$
    Left multiply both sides by $Q^T$
    $$ (Q^{T}Q) (RQ) (Q^{T}v) = Q^{T} \lambda v = \lambda (Q^{T}v)$$ \\
    Set $u = Q^{T} v$, and since $A_1 = RQ$. We get, \\
    $$ A_1 u = \lambda u$$
    Since $\lambda$ and $v$ were arbitrary, we have shown that $A$ and $A_1$ have the same eigenvalues.

\subsection{Find $A_1$ and $A$, verify they have the same eigenvalues}

$$
A = \begin{bmatrix}
1 & 0 \\
1 & 3 
\end{bmatrix} 
$$

    Finding the eigenvalues from factoring the characteristic equation, we get \\
    $$det(A - \lambda I) = (1 - \lambda)(3 - \lambda) = 0 \implies \lambda = 3,1$$ \\

    The QR factorization of A results in $A = QR$ such that \\
    $$
    Q = \begin{bmatrix}
        \frac{1}{\sqrt{2}} & \frac{-3}{\sqrt{18}} \\
        \frac{1}{\sqrt{2}} & \frac{3}{\sqrt{18}} 
        \end{bmatrix},
    R = \begin{bmatrix}
        \sqrt{2} & \frac{3}{\sqrt{2}} \\
        0 & \frac{\sqrt{18}}{2} 
        \end{bmatrix}
    $$ \\
    Therefore, $A_1 = RQ$ is \\
    $$
    A_1 = RQ = 
    \begin{bmatrix}
        \frac{1}{\sqrt{2}} & \frac{-3}{\sqrt{18}} \\
        \frac{1}{\sqrt{2}} & \frac{3}{\sqrt{18}} 
    \end{bmatrix}
    \begin{bmatrix}
        \sqrt{2} & \frac{3}{\sqrt{2}} \\
        0 & \frac{\sqrt{18}}{2} 
    \end{bmatrix}
    = \frac{1}{2}
     \begin{bmatrix}
        5 & 1 \\
        3 & 3 
    \end{bmatrix}
    $$
    Finding the eigenvalues from factoring the characteristic equation of $A_1$, we get \\
    $$det(A_1 - \lambda I) = (\frac{5}{2} - \lambda)(\frac{3}{2} - \lambda) - \frac{3}{2 \times 2} = 0 \implies \lambda^2 -4\lambda +3 = 0  \\
    $$
    $$\implies (1 - \lambda)(3 - \lambda) = 0 \implies \lambda = 3,1$$ \\\\

    Therefore, $A_1$ and $A$ have the same eigenvalues


\subsection{$A_k$ is similar to $A$}

Proof by induction. \\
Inductive Hypothesis: $A_k$ is similar to $A$ for $k \geq 1$ \\
Base case:$A_1$ is similar to $A$ \\
For an invertible matrix Q, \\
$A = QA_{1}Q^T = Q (RQ) Q^T = QR$ \\ \\
Assume inductive hypothesis is true, that $A_k$ is similar to $A$ for $k \geq 1$ \\ \\
To show: $A_{k+1}$ is similar to $A_k$, and therefore, $A$ \\

$A_{k+1} = R_kQ_k$ 

$A_k = Q_k A_{k+1} Q_k^T  =  Q_k R_kQ_k Q_k^T = Q_k R_k$

Clearly, $A_{k+1}$ is similar to $A_k$. Therefore, $A_{k+1}$ is similar to $A_k$ (since $A_k = Q_kR_k = R_{k-1}Q_{k-1}$ and so forth) by the inductive hypothesis.



\subsection{Continuing Problem  1.2}
Python code was written to calculate $A_2, A_3, A_4, A_5$. \\
See attached file \texttt{Q1-4.ipynb}
\\
$$
A_2 = \begin{bmatrix}
3.12 & -0.47 & \\
0.53 & 0.88 & \\
\end{bmatrix}
A_3 = \begin{bmatrix}
3.06 & 0.84 & \\
-0.16 & 0.94 & \\
\end{bmatrix}
A_4 = \begin{bmatrix}
3.02 & -0.95 & \\
0.05 & 0.98 & \\
\end{bmatrix}
A_5 = \begin{bmatrix}
3.01 & 0.98 & \\
-0.02 & 0.99 & \\
\end{bmatrix}
$$ \\ 

As we can clearly see, the diagonals of $A_k$ approach the eigenvalues of $A$ as $k$ increases.


\subsection{Prove Theorem 1.1}
// incomplete

\subsection{QR Algorithm in Python}
    Results from python code (QRAlgorithm(1.6-1.8).py): \\
    \begin{tabular}{c|c}
        \textbf{Matrix} & \textbf{Eigenvalues} \\
        \hline \\
        \(\begin{bmatrix} 2 & 3 \\ 2 & 1 \end{bmatrix}\) & 4.0, -1.0 \\
        \\ \hline \\
        \(\begin{bmatrix} 1 & 1 \\ 2 & 1 \end{bmatrix}\) & 2.41, -0.41 \\
        \\ \hline \\
        \(\begin{bmatrix} 1 & 0 & -1 \\ 1 & 2 & 1 \\ -4 & 0 & 1 \end{bmatrix}\) & 3.01, 1.99, -1.0 \\
        \\ \hline \\
        \(\begin{bmatrix} 1 & 1 & -1 \\ 0 & 2 & 0 \\ -2 & 4 & 2 \end{bmatrix}\) & 3.0, 2.0, 0.0 \\
    \end{tabular}

\subsection{Another Example}
    Results from python code (QRAlgorithm(1.6-1.8).py): \\
    \begin{tabular}{c|c}
        \textbf{Matrix} & \textbf{Eigenvalues} \\
        \hline \\
        \(\begin{bmatrix} 2 & 3 \\ -1 & -2 \end{bmatrix}\) & 2.0, -2.0 \\
    \end{tabular} \\ \\
    This is clearly incorrect as the eigenvalues should be 1 and -1. The basic QR algorithm has failed because the absolute values of the eigenvalues are non-distinct. 
    The QR algorithm relies on seperating eigenvalues based on their magnitudes and since the eigenvalues are the same magnitude, the algorithm fails.
\subsection{Using a Shift}
    Results from python code (QRAlgorithm(1.6-1.8).py): \\
    \begin{tabular}{c|c}
        \textbf{Matrix} & \textbf{Eigenvalues} \\
        \hline \\
        \(\begin{bmatrix} 2 & 3 \\ -1 & -2 \end{bmatrix}\) & 1.0, -1.0 \\
    \end{tabular} \\ \\ \\
$B = A + \alpha I \implies \lambda$ is an eigenvalue of A $\iff \lambda + \alpha$ is an eigenvalue of B. 
\begin{proof}
    $ $\newline
    ($\implies$) Let $\lambda$ be an eigenvalue of A. Then there exists a non-zero vector x such that $Ax = \lambda x$. \\
    Then $Bx = (A + \alpha I)x = Ax + \alpha x = \lambda x + \alpha x = (\lambda + \alpha)x$. \\ 
    Thus, $\lambda + \alpha$ is an eigenvalue of B. \\ \\
    ($\impliedby$) Let $\lambda + \alpha$ be an eigenvalue of B. Then there exists a non-zero vector x such that $Bx = (\lambda + \alpha)x$. \\
    Then $Ax = (B - \alpha I) x = Bx - \alpha I x = (\lambda + \alpha)x - \alpha x = \lambda x$. \\
    Thus, $\lambda$ is an eigenvalue of A. \\ 
\end{proof}


\subsection{The $QR$ factorization of $A^{k+1}$}
\begin{proof}
    $ $\newline
    Let $Q_0 = Q$ and $R_0 = R$. \\
    \textbf{1.} $Q_0 Q_1 \cdots Q_{k-1} A_k = A Q_0 Q_1 \cdots Q_{k-1}$ for all $k \geq 1$ \\ \\
    \textit{Base Case:} $k = 1$ so $A = Q_0 R_0, A_1 = R_0 Q_0$ then, \\
    \indent $Q_0 A_1 = (Q_0 R_0) Q_0 = A Q_0$ \\
    \textit{Inductive Step:} Assume $Q_0 Q_1 \cdots Q_{k-1} A_k = Q_0 Q_1 \cdots Q_{k-1} R_{k-1} Q_{k-1} = A Q_0 Q_1 \cdots Q_{k-1}$, then for $k+1$ \\
    \indent $Q_0 Q_1 \cdots Q_{k-1} Q_k A_{k+1} = Q_0 Q_1 \cdots Q_{k-1} Q_k R_k Q_k$ \\ 
    \indent $= Q_0 Q_1 \cdots Q_{k-1} A_k Q_k$ (Since $A_k = Q_k R_k$) \\
    \indent $= (Q_0 Q_1 \cdots Q_{k-1} R_{k-1} Q_{k-1}) Q_k$ (Since $A_k = R_{k-1} Q_{k-1}$) \\
    \indent $= A Q_0 Q_1 \cdots Q_{k-1} Q_k$ (By Inductive Hypothesis)\\ \\
    Thus $Q_0 Q_1 \cdots Q_{k-1} A_{k} = A Q_0 Q_1 \cdots Q_{k-1}$. \\ \\
    \textbf{2.} $(Q_0 Q_1 \cdots Q_k)(R_k \cdots R_1 R_0) = A(Q_0 Q_1 \cdots Q_{k-1})(R_{k-1} \cdots R_1 R_0)$ \\ \\
    \textit{Base Case:} $k = 1$ so $A = Q_0 R_0, A_1 = R_0 Q_0$ then, \\
    \indent $(Q_0 Q_1)(R_1 R_0) = Q_0 Q_1 R_1 R_0 = Q_0 A_1 R_0$ (Since $A_1 = Q_1 R_1$) \\
    \indent $= Q_0 R_0 Q_0 R_0$ (Since $A_1 = R_0 Q_0$) \\ 
    \indent $= A Q_0 R_0$ (Since $A = Q_0 R_0$) \\
    \textit{Inductive Step:} Assume $(Q_0 Q_1 \cdots Q_k)(R_k \cdots R_1 R_0) = A(Q_0 Q_1 \cdots Q_{k-1})(R_{k-1} \cdots R_1 R_0)$, then for $k+1$ \\
    \indent $(Q_0 Q_1 \cdots Q_k Q_{k+1})(R_{k+1} R_k \cdots R_1 R_0) = Q_0 Q_1 \cdots Q_k Q_{k+1}R_{k+1} R_k \cdots R_1 R_0$ \\
    \indent $= Q_0 Q_1 \cdots Q_k A_{k+1} R_k \cdots R_1 R_0$ (Since $A_{k+1} = Q_{k+1} R_{k+1}$) \\
    \indent $= Q_0 Q_1 \cdots Q_k R_k Q_k R_k \cdots R_1 R_0$ (Since $A_{k+1} = R_k Q_k$) \\
    \indent $= (Q_0 Q_1 \cdots Q_{k-1} A_k) Q_k R_k R_{k-1} \cdots R_1 R_0$ (Since $A_k = Q_k R_k$) \\
    \indent $= (AQ_0 Q_1 \cdots Q_{k-1}) Q_k R_k R_{k-1} \cdots R_1 R_0$ (By \textbf{1.}) \\
    \indent $= A(Q_0 Q_1 \cdots Q_{k-1} Q_k) (R_k R_{k-1} \cdots R_1 R_0)$ \\ \\
    Thus $(Q_0 Q_1 \cdots Q_k)(R_k \cdots R_1 R_0) = A(Q_0 Q_1 \cdots Q_{k-1})(R_{k-1} \cdots R_1 R_0)$. \\ \\
    \textbf{3.} $A^{k+1} = (Q_0 Q_1 \cdots Q_k) (R_k \cdots R_1 R_0)$ \\ \\
    \textit{Base Case:} $k = 0$ so $A^1 = Q_0 R_0 = QR = A$ \\
    \textit{Inductive Step:} Assume $A^{k+1} = (Q_0 Q_1 \cdots Q_k) (R_k \cdots R_1 R_0)$, then for $k+2$ \\
    \indent $A^{k+2} = A A^{k+1} = A (Q_0 Q_1 \cdots Q_k) (R_k \cdots R_1 R_0)$ (By Inductive Hypothesis) \\
    \indent $= (Q_0 Q_1 \cdots Q_kQ_{k+1})(R_{k+1}R{k} \cdots R_1 R_0)$ (By \textbf{2.}) \\ \\
    Thus $A^{k+1} = (Q_0 Q_1 \cdots Q_k) (R_k \cdots R_1 R_0)$ for all $k \geq 0$.
\end{proof}

\subsection{2.1 R}

\section{Image Compression}
\subsection{The Outer Product Form of the SVD}

\begin{enumerate}[label=(\alph*)]
\item Prove theorem 2.1

We have $A = U\Sigma V^T$, the SVD Decomposition of A \\
Let  $U = \begin{bmatrix} u_1 & u_2 & ... & u_r & u_{r+1} & ... & u_m\end{bmatrix}$ and, \\

Let $V = \begin{bmatrix} v_1 & v_2 & ... & v_r & v_{r+1} & ... & v_n\end{bmatrix}$ \\

Therefore, $V^T = \begin{bmatrix} v_1^T \\ v_2^T \\ ... \\ v_r^T \\ v_{r+1}^T \\ ... \\ v_n^T\end{bmatrix} $ \\ 

We want to show that \\
$A = \sigma_{1}u_{1}v_{1}^T + \sigma_{2}u_{2}v_{2}^T + ... + \sigma_{r}u_{r}v_{r}^T$\\

$$ U\Sigma V^T = \begin{bmatrix} u_1 & u_2 & ... & u_r & u_{r+1} & ... & u_m\end{bmatrix} \begin{bmatrix}
    \begin{bmatrix}
        \sigma_1 & 0 & ... & 0 \\
        0 & \sigma_2 & ... & 0 \\
        0 & 0 & \ddots & 0 \\
        0 & 0 & ... & \sigma_r
    \end{bmatrix} & 0 \\
    0 & 0
\end{bmatrix} \begin{bmatrix} v_1^T \\ v_2^T \\ ... \\ v_r^T \\ v_{r+1}^T \\ ... \\ v_n^T\end{bmatrix}$$

$$\implies U\Sigma V^T = \begin{bmatrix} \sigma_{1}u_1 & \sigma_{2}u_2 & ... & \sigma_{r}u_r & (0)u_{r+1} & ... & (0)u_n\end{bmatrix} \begin{bmatrix} v_1^T \\ v_2^T \\ ... \\ v_r^T \\ v_{r+1}^T \\ ... \\ v_n^T\end{bmatrix} $$

$$\implies U\Sigma V^T = \sigma_{1}u_1v_1^T + \sigma_{2}u_2v_2^T + ... + \sigma_{r}u_rv_r^T + (0)u_{r+1}v_{r+1}^T + ... + (0)u_nv_n^T $$

$$ \implies A = U\Sigma V^T = \sigma_{1}u_1v_1^T + \sigma_{2}u_2v_2^T + ... + \sigma_{r}u_rv_r^T$$ QED

\item $ A = \begin{bmatrix}
    -2 & 2 \\
    -1 & 1 \\ 
    2 & -2 
\end{bmatrix} $ \\

$A^TA = \begin{bmatrix}
    -2 & -1 & 2 \\ 
    2 & 1 & -2
\end{bmatrix}\begin{bmatrix}
    -2 & 2 \\
    -1 & 1 \\ 
    2 & -2 
\end{bmatrix} = \begin{bmatrix}
    9 & -9 \\ 
    -9 & 9
\end{bmatrix}$ \\

Solving the characteristic equation, $det(A^TA - \lambda I) = 0$ \\
$(9-\lambda)^2 - (-9)^2 = 0 \iff 81 + \lambda^2 -18\lambda - 81 = 0 \iff \lambda^2 - 18\lambda = 0$ \\
$\lambda = 18,0$ \\
Finding the respective eigenvectors, \\
Solving $A^TA - \lambda I = 0$
For $\lambda = 18$ \\
$A^TA - 18I = \begin{bmatrix}
    -9 & -9 \\ 
    -9 & -9
\end{bmatrix} $\\

$\implies -9x_1 = 9x_2 \implies x_1 = -x_2 \implies v_1^{'} = x_2 \begin{bmatrix}
-1 \\ 1
\end{bmatrix}$ \\
$\implies v_1 = \begin{bmatrix}
\frac{-1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{bmatrix} $ \\

For $\lambda = 0$ \\
$A^TA - 0I = \begin{bmatrix}
    9 & -9 \\ 
    -9 & 9
\end{bmatrix} $\\

$\implies 9x_1 = 9x_2 \implies x_1 = x_2 \implies v_2^{'} = x_2 \begin{bmatrix}
1 \\ 1
\end{bmatrix}$ \\

$\implies v_2 = \begin{bmatrix}
\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix} $ \\

Therefore,
$ V = [v_1 v_2] = \begin{bmatrix}
    \frac{-1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ 
    \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} 
\end{bmatrix} \implies V^T = \begin{bmatrix}
    \frac{-1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ 
    \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} 
\end{bmatrix} $ \\

And, $\Sigma = \begin{bmatrix}
    \sqrt{18} & 0 \\ 0 & 0 \\ 0 & 0
\end{bmatrix}$ \\

Finally, we need to calculate $U$ \\
Since $r=1$, we can only calculate one left singular value \\ 
$u_1 = \frac{1}{\sigma_1} Av_1 = \frac{1}{\sqrt{18}} \begin{bmatrix}
    -2 & 2 \\
    -1 & 1 \\ 
    2 & -2 
\end{bmatrix} \begin{bmatrix}
\frac{-1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{bmatrix} = \frac{1}{\sqrt{18}}\begin{bmatrix}
    \frac{4}{\sqrt{2}} \\
    \frac{2}{\sqrt{2}} \\ 
    \frac{-4}{\sqrt{2}}
\end{bmatrix} = \begin{bmatrix}
    \frac{4}{6} \\
    \frac{2}{6} \\ 
    \frac{-4}{6}
\end{bmatrix} = \begin{bmatrix}
    2/3 \\ 1/3 \\ -2/3
\end{bmatrix}$   \\

Need $x$ such that $x \cdot u_1 = 0$ \\
$\frac{2}{3} x_1 + \frac{1}{3}x_2 - \frac{2}{3}x_3 = 0 \implies 2x_1 + x_2 = 2x_3 \implies x_3 = \frac{1}{2}x_2 + x_1 $ \\
$\implies x = \begin{bmatrix}
    x_1 \\ x_2 \\ x_3 
\end{bmatrix} =  x_1\begin{bmatrix}
   1 \\ 0 \\ 1
\end{bmatrix} + x_2\begin{bmatrix}
   0 \\ 1 \\ 1/2
\end{bmatrix}$ \\
Therefore we have,
$w_1 =  \begin{bmatrix}
   1 \\ 0 \\ 1
\end{bmatrix}, w_2= \begin{bmatrix}
   0 \\ 1 \\ 1/2
\end{bmatrix}$ \\
Applying Gram-Schmidt, \\

$u_2^{'} =  w_1$ \\
$u_2 = \frac{u_2^{'}}{||u_2^{'}||} = \begin{bmatrix}
   \frac{1}{\sqrt{2}}\\ 0 \\ \frac{1}{\sqrt{2}}
\end{bmatrix} $

$u_3^{'} = w_2 - (w_2 \cdot u_2) u_2  = \begin{bmatrix}
   0 \\ 1 \\ 1/2
\end{bmatrix} - (0 + 0 + \frac{1}{2\sqrt{2}}) \begin{bmatrix}
   \frac{1}{\sqrt{2}}\\ 0 \\ \frac{1}{\sqrt{2}}
\end{bmatrix} $ \\
$ = \begin{bmatrix}
   0 \\ 1 \\ 1/2
\end{bmatrix} -  \begin{bmatrix}
   1/4 \\ 0 \\ 1/4 
\end{bmatrix}  = \begin{bmatrix}
   -1/4 \\ 1 \\ 1/4 
\end{bmatrix} $ \\
$u_3 = \frac{u_3^{'}}{||u_3^{'}||}  = \begin{bmatrix}
    \frac{-1}{3\sqrt{2}} \\ \frac{2\sqrt{2}}{3} \\ \frac{1}{3\sqrt{2}}
\end{bmatrix}$ \\

Then, we have $U = [u_1 u_2 u_3] = \begin{bmatrix}
    2/3 & \frac{1}{\sqrt{2}} & \frac{-1}{3\sqrt{2}} \\
    1/3 & 0 & \frac{2\sqrt{2}}{3} \\
    -2/3 & \frac{1}{\sqrt{2}} & \frac{1}{3\sqrt{2}}
\end{bmatrix}$ \\
\\ 
Finally, we have \\

$A = \begin{bmatrix}
    -2 & 2 \\
    -1 & 1 \\ 
    2 & -2 
\end{bmatrix} = U\Sigma V^T = \begin{bmatrix}
    2/3 & \frac{1}{\sqrt{2}} & \frac{-1}{3\sqrt{2}} \\
    1/3 & 0 & \frac{2\sqrt{2}}{3} \\
    -2/3 & \frac{1}{\sqrt{2}} & \frac{1}{3\sqrt{2}}
\end{bmatrix} \begin{bmatrix}
    \sqrt{18} & 0 \\ 0 & 0 \\ 0 & 0
\end{bmatrix} \begin{bmatrix}
    \frac{-1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ 
    \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} 
\end{bmatrix}$

And, 
$u_1 = \begin{bmatrix}
    2/3 \\ 1/3 \\ -2/3
\end{bmatrix}$ \\
$\sigma_1 = \sqrt{18}$ \\
$v_1^{T} = \begin{bmatrix}
    -1/\sqrt{2} & 1/\sqrt{2}
\end{bmatrix}$ \\ 

Calculating the product, we see
$\sigma_1 u_1 v_1^T = \begin{bmatrix}
    -2 & 2 \\
    -1 & 1 \\ 
    2 & -2 
\end{bmatrix} = A$ \\
As expected.
\end{enumerate}

\subsection{Digital Image Compression}
See attached file \texttt{Q2-2.ipynb}
\end{document}
